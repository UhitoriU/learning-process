{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from zhipuai_embedding import ZhipuAIEmbeddings\n",
    "\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "zhipuai_api_key = os.environ['ZHIPUAI_API_KEY']\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "embedding = ZhipuAIEmbeddings()\n",
    "\n",
    "persist_directory = '../data_base/vector_db/chroma'\n",
    "\n",
    "vectordb = Chroma(\n",
    "    persist_directory=persist_directory,  \n",
    "    embedding_function=embedding\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model_name = \"gpt-3.5-turbo\", temperature = 0,openai_api_key=os.environ['OPENAI_API_KEY'],openai_api_base=os.environ['BASE_URL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "template_v1 = \"\"\"使用以下上下文来回答最后的问题。如果你不知道答案，就说你不知道，不要试图编造答\n",
    "案。最多使用三句话。尽量使答案简明扼要。总是在回答的最后说“谢谢你的提问！”。\n",
    "{context}\n",
    "问题: {question}\n",
    "\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\",\"question\"],\n",
    "                                 template=template_v1)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm,\n",
    "                                       retriever=vectordb.as_retriever(),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type_kwargs={\"prompt\":QA_CHAIN_PROMPT})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16333/2891710412.py:2: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"query\": question})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "南瓜书是一本解析《机器学习》（西瓜书）中难以理解公式的书籍。\n",
      "谢谢你的提问！\n"
     ]
    }
   ],
   "source": [
    "question = \"什么是南瓜书\"\n",
    "result = qa_chain({\"query\": question})\n",
    "print(result[\"result\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "南瓜书是一本机器学习领域的补充教材，旨在对《机器学习》（西瓜书）中难以理解的公式进行解析，并补充推导细节。它的内容是以西瓜书的内容为基础，提供了更详细的数学推导和解释，帮助读者更好地理解和掌握机器学习算法原理。在阅读南瓜书时，最佳的使用方法是将西瓜书作为主线，遇到推导困难或理解不清的公式时再来查阅南瓜书，以加深对机器学习理论的理解。\n"
     ]
    }
   ],
   "source": [
    "template_v2 = \"\"\"使用以下上下文来回答最后的问题。如果你不知道答案，就说你不知道，不要试图编造答\n",
    "案。你应该使答案尽可能详细具体，但不要偏题。如果答案比较长，请酌情进行分段，以提高答案的阅读体验。\n",
    "{context}\n",
    "问题: {question}\n",
    "有用的回答:\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\",\"question\"],\n",
    "                                 template=template_v2)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm,\n",
    "                                       retriever=vectordb.as_retriever(),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type_kwargs={\"prompt\":QA_CHAIN_PROMPT})\n",
    "\n",
    "question = \"什么是南瓜书\"\n",
    "result = qa_chain({\"query\": question})\n",
    "print(result[\"result\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "构造 Prompt 时，使用大模型时的原则主要包括以下几点：\n",
      "\n",
      "1. 确定明确的目标和任务：在构造 Prompt 时，需要明确定义任务的目标和内容，确保 Prompt 能够准确引导模型产生预期的输出。\n",
      "\n",
      "2. 简洁明了的提示信息：Prompt 应该简洁明了，包含足够的关键信息，但不要过于复杂或含糊，以避免模型产生错误的理解或输出。\n",
      "\n",
      "3. 规范化 Prompt 结构：在构造 Prompt 时，最好使用规范化的结构和语法，以便模型能够更好地理解和处理输入的信息。\n",
      "\n",
      "4. 考虑语境和背景知识：根据任务的语境和需要的背景知识，设计相应的 Prompt，以提高模型对输入信息的理解和处理能力。\n",
      "\n",
      "总之，构造 Prompt 时应该确保其清晰、简洁、规范化，并考虑任务的要求和背景知识，以帮助模型准确理解并产生正确的输出。\n"
     ]
    }
   ],
   "source": [
    "question = \"使用大模型时，构造 Prompt 的原则有哪些\"\n",
    "result = qa_chain({\"query\": question})\n",
    "print(result[\"result\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对于问题使用大模型时构造Prompt的原则，可以参考以下几点原则：\n",
      "\n",
      "1. 简洁明了：Prompt应该是简洁明了的，能够清晰地指导模型进行任务的解决，避免出现歧义或多余信息。\n",
      "   \n",
      "2. 相关性：Prompt应该与任务或问题直接相关，能够帮助模型理解并正确解决任务，不应包含无关或冗余信息。\n",
      "   \n",
      "3. 信息量：Prompt应该包含足够的信息量，能够准确指导模型完成任务，同时不应过于复杂或繁琐，以免混淆模型。\n",
      "   \n",
      "4. 正向引导：Prompt应该以正向的方式引导模型进行任务解决，避免模糊或否定性的表达，以提高模型理解和学习效果。\n",
      "\n",
      "以上是构造Prompt时可以考虑的一些原则，通过合理设计Prompt可以有效提高模型的学习和表现效果。如果想要进一步了解构造Prompt的方法和技巧，可以参考相关的文献和实践经验。\n"
     ]
    }
   ],
   "source": [
    "template_v3 = \"\"\"使用以下上下文来回答最后的问题。如果你不知道答案，就说你不知道，不要试图编造答\n",
    "案。你应该使答案尽可能详细具体，但不要偏题。如果答案比较长，请酌情进行分段，以提高答案的阅读体验。\n",
    "如果答案有几点，你应该分点标号回答，让答案清晰具体\n",
    "{context}\n",
    "问题: {question}\n",
    "有用的回答:\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\",\"question\"],\n",
    "                                 template=template_v3)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm,\n",
    "                                       retriever=vectordb.as_retriever(),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type_kwargs={\"prompt\":QA_CHAIN_PROMPT})\n",
    "\n",
    "question = \"使用大模型时，构造 Prompt 的原则有哪些\"\n",
    "result = qa_chain({\"query\": question})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "强化学习是一种机器学习的范式，其目标是通过代理与环境的交互来学习最优决策策略。在强化学习中，代理根据环境的反馈，通过试错的方式来学习如何采取行动以最大化累积奖励。强化学习与监督学习和无监督学习不同，它更注重在不断与环境的交互中学习并提升策略。强化学习通常涉及一个状态空间、一个动作空间、一个奖励信号和一个学习策略。\n",
      "\n",
      "在强化学习中，一个关键概念是马尔可夫决策过程（MDP），它是一个描述强化学习环境的数学框架。MDP包括状态空间、动作空间、转移概率、奖励函数和折扣因子等要素。代理根据当前状态选择动作，并且根据环境的反馈获得奖励，学习如何在不同状态下选择最优的动作。强化学习的目标是找到一个最优策略，使得代理在长期累积奖励最大化。\n",
      "\n",
      "在实际应用中，强化学习被广泛应用于许多领域，如游戏、机器人控制、金融交易等。通过强化学习，代理可以在复杂的环境中学习并逐渐改进策略，实现自主决策和优化目标。强化学习是机器智能领域的一个重要研究方向，为实现人工智能的智能体提供了强大的学习方法。\n"
     ]
    }
   ],
   "source": [
    "#标明知识来源，提高可信度\n",
    "\n",
    "#大模型存在幻觉问题，有时回答并非源于知识库内容，例如\n",
    "question = \"强化学习的定义是什么\"\n",
    "result = qa_chain({\"query\": question})\n",
    "print(result[\"result\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "强化学习是一种机器学习方法，其目标是通过与环境的交互来学习如何做出决策，以使某种累积奖励最大化。在强化学习中，代理根据环境的状态采取行动，并根据行动后获得的奖励来调整行为策略。强化学习与监督学习和无监督学习相比，更注重通过试错来学习最优决策。\n",
      "\n",
      "来源：南瓜书，第3章，3.6小节。\n"
     ]
    }
   ],
   "source": [
    "template_v4 = \"\"\"使用以下上下文来回答最后的问题。如果你不知道答案，就说你不知道，不要试图编造答\n",
    "案。你应该使答案尽可能详细具体，但不要偏题。如果答案比较长，请酌情进行分段，以提高答案的阅读体验。\n",
    "如果答案有几点，你应该分点标号回答，让答案清晰具体。\n",
    "请你附上回答的来源原文，以保证回答的正确性。\n",
    "{context}\n",
    "问题: {question}\n",
    "有用的回答:\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\",\"question\"],\n",
    "                                 template=template_v4)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm,\n",
    "                                       retriever=vectordb.as_retriever(),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type_kwargs={\"prompt\":QA_CHAIN_PROMPT})\n",
    "\n",
    "question = \"强化学习的定义是什么\"\n",
    "result = qa_chain({\"query\": question})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根据提供的上下文信息来看，提到了模型评估与选择的内容，包括经验误差和过拟合、评估方法、性能度量等。而具体到构造一个LLM项目的内容并没有在给出的上下文中提到，因此无法直接从这段文字中得出如何构造一个LLM项目的答案。\n",
      "\n",
      "因此，根据提供的上下文信息，无法回答如何构造一个LLM项目的问题。需要更多的具体信息或上下文才能提供有关LLM项目构造的有效回答。\n"
     ]
    }
   ],
   "source": [
    "#构造思维链\n",
    "question = \"我们应该如何去构造一个LLM项目\"\n",
    "result = qa_chain({\"query\": question})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根据提供的上下文，不能确定如何构造一个LLM项目的具体步骤。因为上下文中主要涉及模型评估与选择、决策树、线性模型等内容，没有提供关于LLM项目构造的相关信息。因此，无法提供准确的回答。\n"
     ]
    }
   ],
   "source": [
    "template_v4 = \"\"\"\n",
    "请你依次执行以下步骤：\n",
    "① 使用以下上下文来回答最后的问题。如果你不知道答案，就说你不知道，不要试图编造答案。\n",
    "你应该使答案尽可能详细具体，但不要偏题。如果答案比较长，请酌情进行分段，以提高答案的阅读体验。\n",
    "如果答案有几点，你应该分点标号回答，让答案清晰具体。\n",
    "上下文：\n",
    "{context}\n",
    "问题: \n",
    "{question}\n",
    "有用的回答:\n",
    "② 基于提供的上下文，反思回答中有没有不正确或不是基于上下文得到的内容，如果有，回答你不知道\n",
    "确保你执行了每一个步骤，不要跳过任意一个步骤。\n",
    "\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\",\"question\"],\n",
    "                                 template=template_v4)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm,\n",
    "                                       retriever=vectordb.as_retriever(),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type_kwargs={\"prompt\":QA_CHAIN_PROMPT})\n",
    "\n",
    "question = \"我们应该如何去构造一个LLM项目\"\n",
    "result = qa_chain({\"query\": question})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根据上下文可以看出，LLM的分类是什么是一个问题，需要返回一个Python List。根据给定的上下文，没有提供关于LLM的分类的信息，因此无法准确回答这个问题。我不知道LLM的分类是什么，无法提供准确的Python List。\n"
     ]
    }
   ],
   "source": [
    "#增加一个指令解析\n",
    "question = \"LLM的分类是什么？给我返回一个 Python List\"\n",
    "result = qa_chain({\"query\": question})\n",
    "print(result[\"result\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    base_url=os.environ[\"BASE_URL\"]\n",
    ")\n",
    "\n",
    "\n",
    "def gen_gpt_messages(prompt):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    return messages\n",
    "\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\", temperature = 0):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=gen_gpt_messages(prompt),\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    if len(response.choices) > 0:\n",
    "        return response.choices[0].message.content\n",
    "    return \"generate answer error\"\n",
    "\n",
    "prompt_input = '''\n",
    "请判断以下问题中是否包含对输出的格式要求，并按以下要求输出：\n",
    "请返回给我一个可解析的Python列表，列表第一个元素是对输出的格式要求，应该是一个指令；第二个元素是去掉格式要求的问题原文\n",
    "如果没有格式要求，请将第一个元素置为空\n",
    "需要判断的问题：\n",
    "~~~\n",
    "{}\n",
    "~~~\n",
    "不要输出任何其他内容或格式，确保返回结果可解析。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n[\\n    \"给我返回一个 Python List\",\\n    \"LLM的分类是什么？\"\\n]'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = get_completion(prompt_input.format(question))\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_output = '''\n",
    "请根据回答文本和输出格式要求，按照给定的格式要求对问题做出回答\n",
    "需要回答的问题：\n",
    "~~~\n",
    "{}\n",
    "~~~\n",
    "回答文本：\n",
    "~~~\n",
    "{}\n",
    "~~~\n",
    "输出格式要求：\n",
    "~~~\n",
    "{}\n",
    "~~~\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['我不知道LLM的分类是什么']\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = 'LLM的分类是什么？给我返回一个 Python List'\n",
    "# 首先将格式要求与问题拆分\n",
    "input_lst_s = get_completion(prompt_input.format(question))\n",
    "# 找到拆分之后列表的起始和结束字符\n",
    "start_loc = input_lst_s.find('[')\n",
    "end_loc = input_lst_s.find(']')\n",
    "rule, new_question = eval(input_lst_s[start_loc:end_loc+1])\n",
    "# 接着使用拆分后的问题调用检索链\n",
    "result = qa_chain({\"query\": new_question})\n",
    "result_context = result[\"result\"]\n",
    "# 接着调用输出格式解析\n",
    "response = get_completion(prompt_output.format(new_question, result_context, rule))\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-universe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
